{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8e056a",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3413cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3 \n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1cc4d",
   "metadata": {},
   "source": [
    "### \"Pre processing\" the data \n",
    "it would be great to have the file path with it in each data frame\n",
    "(this is nearly identical to what Anna is doing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098aa8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('content\\ground_truth\\data.xlsx')\n",
    "ground_truth_df = df[['Study_ID', 'Study', 'Allocation', 'Experimenter']]\n",
    "ground_truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data from the csv files in the extracted directory\n",
    "\n",
    "extracted_dir = 'content/extracted'\n",
    "extracted_dfs = []\n",
    "\n",
    "if os.path.exists(extracted_dir):\n",
    "    for filename in os.listdir(extracted_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(extracted_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                extracted_dfs.append(df)\n",
    "                print(f\"Loaded {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {filename}: {e}\")\n",
    "\n",
    "if extracted_dfs:\n",
    "    extracted_combined_df = pd.concat(extracted_dfs, ignore_index=True)\n",
    "    print(\"Combined DataFrame created.\")\n",
    "else:\n",
    "    print(\"No CSV files found in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly taken from Anna's code\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def format_studyName(study_name_string):\n",
    "    \"\"\"\n",
    "    Cuts a string after the last four-digit number, assuming it represents the year.\n",
    "\n",
    "    Args:\n",
    "        study_name_string (str): The input string potentially containing a year.\n",
    "\n",
    "    Returns:\n",
    "        str: The string cut after the year, or the original string if no year is found.\n",
    "    \"\"\"\n",
    "    # Get rid of all the points, -\n",
    "    study_name_string = study_name_string.replace('.', '')\n",
    "    study_name_string = study_name_string.replace(',', '')\n",
    "    study_name_string = study_name_string.replace(' - ', ' ')\n",
    "    study_name_string = study_name_string.replace(')', '')\n",
    "    study_name_string = study_name_string.replace('(', '')\n",
    "    study_name_string = study_name_string.replace('&', 'and')\n",
    "    study_name_string = remove_accents(study_name_string)\n",
    "    # Find all occurrences of four consecutive digits (potential years)\n",
    "    year_matches = list(re.finditer(r'\\b\\d{4}\\b', study_name_string))\n",
    "\n",
    "    if year_matches:\n",
    "        # Get the last match\n",
    "        last_year_match = year_matches[-1]\n",
    "        # Get the end index of the last year match\n",
    "        end_of_year_index = last_year_match.end()\n",
    "        # Slice the string up to the end of the year\n",
    "        cut_string = study_name_string[:end_of_year_index]\n",
    "        return cut_string.strip() # Use strip to remove trailing whitespace\n",
    "    else:\n",
    "        # If no four-digit number is found, return the original string\n",
    "        return study_name_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dff752",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_combined_df['Study'] = extracted_combined_df['Study'].apply(format_studyName)\n",
    "extracted_combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76309230",
   "metadata": {},
   "source": [
    "#### To group the tables by Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d594df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided by Anna \n",
    "def accuracy_check(col_name, df_extracted, test_table):\n",
    "  allocation_match = False\n",
    "  experimenter_match = False\n",
    "\n",
    "  # Check if both dataframes have the expected columns and rows\n",
    "  if col_name in df_extracted.columns and \\\n",
    "    not df_extracted.empty and not test_table.empty:\n",
    "\n",
    "      extracted_allocation = df_extracted[col_name].iloc[0]\n",
    "\n",
    "      ground_truth_allocation = test_table[col_name].iloc[0]\n",
    "\n",
    "      # Simple case-insensitive comparison\n",
    "      if str(extracted_allocation).lower() == str(ground_truth_allocation).lower():\n",
    "          allocation_match = True\n",
    "          print(f'{col_name}: Match')\n",
    "      else:\n",
    "          print(f\"{col_name}: Mismatch (Extracted: '{extracted_allocation}', Ground Truth: '{ground_truth_allocation}')\")\n",
    "  else:\n",
    "    print(\"Cannot perform accuracy check: Extracted or ground truth data is missing or malformed.\")\n",
    "  print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44c39e",
   "metadata": {},
   "source": [
    "## Calculating the metrics\n",
    "Auxilary functions used to calculate the metrics just so it can be easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : it would be great to have a class that has all those at the same time\n",
    "\n",
    "def calculateAccuracy(TP, FP, FN):\n",
    "    '''calculates the accuracy of a model based on true positives, true negatives, false positives, and false negatives.'''\n",
    "    return TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
    "\n",
    "def calculatePrecision(TP, FP):\n",
    "    '''calculates the precision of a model based on true positives and false positives.'''\n",
    "    return TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "def calculateRecall(TP, FN):\n",
    "    '''calculates the recall of a model based on true positives and false negatives.'''\n",
    "    return TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "def calculateF1Score(precision, recall):\n",
    "    '''calculates the F1 score based on precision and recall.'''\n",
    "    return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "def calculateMetrics(TP, FP, FN):\n",
    "    '''calculates various metrics based on true positives, false positives, and false negatives.'''\n",
    "    # --- Metrics ---\n",
    "    accuracy = calculateAccuracy(TP, FP, FN) \n",
    "    recall = calculateRecall(TP, FN)\n",
    "    precision = calculatePrecision(TP, FP)\n",
    "    f1 = calculateF1Score(precision, recall)\n",
    "\n",
    "    return {\n",
    "        'Accuracy': round(accuracy, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'F1': round(f1, 4)\n",
    "    }\n",
    "\n",
    "#testing the metrics\n",
    "'''\n",
    "Example usage of the calculateMetrics function\n",
    "metrics = calculateMetrics(10, 5, 2)\n",
    "print(metrics)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3436058",
   "metadata": {},
   "source": [
    "### Exparimenter\n",
    "basically it's when the selected column can be more then 2 values (non binary), i think it can work with allocation (binary values) but i still did a separate case for it just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided by Anna \n",
    "def accuracy_check(col_name, df_extracted, test_table):\n",
    "  allocation_match = False\n",
    "  experimenter_match = False\n",
    "\n",
    "  # Check if both dataframes have the expected columns and rows\n",
    "  if col_name in df_extracted.columns and \\\n",
    "    not df_extracted.empty and not test_table.empty:\n",
    "\n",
    "      extracted_allocation = df_extracted[col_name].iloc[0]\n",
    "\n",
    "      ground_truth_allocation = test_table[col_name].iloc[0]\n",
    "\n",
    "      # Simple case-insensitive comparison\n",
    "      if str(extracted_allocation).lower() == str(ground_truth_allocation).lower():\n",
    "          allocation_match = True\n",
    "          print(f'{col_name}: Match')\n",
    "      else:\n",
    "          print(f\"{col_name}: Mismatch (Extracted: '{extracted_allocation}', Ground Truth: '{ground_truth_allocation}')\")\n",
    "  else:\n",
    "    print(\"Cannot perform accuracy check: Extracted or ground truth data is missing or malformed.\")\n",
    "  print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2df65",
   "metadata": {},
   "source": [
    "# To do :\n",
    "nexts steps are to add graphics\n",
    "- check what are the appropriate graphics to match this data\n",
    "- fjhjdfv zbel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d7277",
   "metadata": {},
   "source": [
    "# Plots \n",
    "plotting the outputted numbers uwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f6ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for Allocation (binary)\n",
    "allocation_metrics = calculateMetrics(TP, FP, FN)\n",
    "\n",
    "# Metrics for Experimenter (multi-class)\n",
    "experimenter_metrics = calculateMetrics(TPE_sum, FPE_sum, FNE_sum)\n",
    "\n",
    "# Prepare data for plotting\n",
    "labels = ['Accuracy', 'Recall', 'Precision', 'F1']\n",
    "allocation_values = [allocation_metrics[label] for label in labels]\n",
    "experimenter_values = [experimenter_metrics[label] for label in labels]\n",
    "\n",
    "x = range(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar([i - width/2 for i in x], allocation_values, width, label='Allocation')\n",
    "plt.bar([i + width/2 for i in x], experimenter_values, width, label='Experimenter')\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.title('Evaluation Metrics')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
